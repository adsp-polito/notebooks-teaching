{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning for Image Classification\n",
    "\n",
    "This notebook demonstrates transfer learning using a pre-trained ResNet model for image classification on the CIFAR-10 dataset.\n",
    "\n",
    "## What is Transfer Learning?\n",
    "\n",
    "Transfer learning is a machine learning technique where a model trained on one task is repurposed for a second related task. It leverages knowledge gained from solving one problem and applies it to a different but related problem.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "1. **Feature Extraction**: Use pre-trained model as fixed feature extractor\n",
    "2. **Fine-tuning**: Unfreeze some layers and train them on new data\n",
    "3. **Domain Adaptation**: Adapt model from one domain to another\n",
    "\n",
    "## Benefits\n",
    "\n",
    "- **Faster Training**: Start with learned features instead of random initialization\n",
    "- **Less Data Required**: Pre-trained features work well even with limited data\n",
    "- **Better Performance**: Often achieves higher accuracy than training from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10 class names\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Data augmentation and normalization for training\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                       std=[0.229, 0.224, 0.225])  # ImageNet statistics\n",
    "])\n",
    "\n",
    "# Just normalization for validation/test\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                       std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "print('Loading CIFAR-10 dataset...')\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, \n",
    "                                 download=True, transform=train_transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False,\n",
    "                                download=True, transform=test_transform)\n",
    "\n",
    "# Create a smaller training set for faster demo\n",
    "train_size = 5000  # Use 5000 samples instead of full 50000\n",
    "train_dataset, _ = random_split(train_dataset, [train_size, len(train_dataset) - train_size])\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                         shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                        shuffle=False, num_workers=2)\n",
    "\n",
    "print(f'Training samples: {len(train_dataset)}')\n",
    "print(f'Test samples: {len(test_dataset)}')\n",
    "print(f'Number of classes: {len(class_names)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to denormalize images for visualization\n",
    "def denormalize(tensor, mean, std):\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return tensor\n",
    "\n",
    "# Get a batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Display images\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "std = torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = images[i].clone()\n",
    "    img = denormalize(img, mean, std)\n",
    "    img = torch.clamp(img, 0, 1)\n",
    "    ax.imshow(img.permute(1, 2, 0))\n",
    "    ax.set_title(class_names[labels[i]])\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-trained ResNet Model\n",
    "\n",
    "We'll use ResNet-18, a popular convolutional neural network pre-trained on ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained ResNet-18\n",
    "print('Loading pre-trained ResNet-18...')\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Print model architecture\n",
    "print('\\nModel Architecture:')\n",
    "print(model)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'\\nTotal parameters: {total_params:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning Strategy 1: Feature Extraction\n",
    "\n",
    "In this approach, we freeze all convolutional layers and only train the final classification layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature extraction model\n",
    "def create_feature_extractor():\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    \n",
    "    # Freeze all layers\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Replace the final fully connected layer\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, len(class_names))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "feature_extractor = create_feature_extractor().to(device)\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable_params = sum(p.numel() for p in feature_extractor.parameters() if p.requires_grad)\n",
    "print(f'Trainable parameters: {trainable_params:,}')\n",
    "print(f'Frozen parameters: {total_params - trainable_params:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=5):\n",
    "    \"\"\"\n",
    "    Train the model and track performance\n",
    "    \"\"\"\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'test_acc': []\n",
    "    }\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 50)\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        for inputs, labels in tqdm(train_loader, desc='Training'):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "        \n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_acc'].append(epoch_acc.item())\n",
    "        \n",
    "        # Evaluation phase\n",
    "        test_acc = evaluate_model(model, test_loader)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        \n",
    "        print(f'Train Loss: {epoch_loss:.4f} | Train Acc: {epoch_acc:.4f}')\n",
    "        print(f'Test Acc: {test_acc:.4f}\\n')\n",
    "        \n",
    "        # Save best model\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, history\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    \"\"\"\n",
    "    Evaluate model accuracy\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Feature Extractor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(feature_extractor.fc.parameters(), lr=0.001)\n",
    "\n",
    "# Train\n",
    "print('Training Feature Extractor Model...\\n')\n",
    "feature_extractor, history_fe = train_model(\n",
    "    feature_extractor, \n",
    "    train_loader, \n",
    "    test_loader,\n",
    "    criterion, \n",
    "    optimizer, \n",
    "    num_epochs=5\n",
    ")\n",
    "\n",
    "print(f'Best Test Accuracy (Feature Extraction): {max(history_fe[\"test_acc\"]):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning Strategy 2: Fine-tuning\n",
    "\n",
    "In this approach, we unfreeze some layers and train them along with the classification layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fine-tuning model\n",
    "def create_finetuning_model():\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    \n",
    "    # Freeze early layers\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'layer4' not in name and 'fc' not in name:\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    # Replace the final fully connected layer\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, len(class_names))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "finetuning_model = create_finetuning_model().to(device)\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable_params = sum(p.numel() for p in finetuning_model.parameters() if p.requires_grad)\n",
    "print(f'Trainable parameters: {trainable_params:,}')\n",
    "print(f'Frozen parameters: {total_params - trainable_params:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Fine-tuning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training with different learning rates for different layers\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam([\n",
    "    {'params': finetuning_model.layer4.parameters(), 'lr': 1e-4},\n",
    "    {'params': finetuning_model.fc.parameters(), 'lr': 1e-3}\n",
    "])\n",
    "\n",
    "# Train\n",
    "print('Training Fine-tuning Model...\\n')\n",
    "finetuning_model, history_ft = train_model(\n",
    "    finetuning_model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    num_epochs=5\n",
    ")\n",
    "\n",
    "print(f'Best Test Accuracy (Fine-tuning): {max(history_ft[\"test_acc\"]):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Training accuracy\n",
    "axes[0].plot(history_fe['train_acc'], marker='o', label='Feature Extraction')\n",
    "axes[0].plot(history_ft['train_acc'], marker='s', label='Fine-tuning')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Training Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Test accuracy\n",
    "axes[1].plot(history_fe['test_acc'], marker='o', label='Feature Extraction')\n",
    "axes[1].plot(history_ft['test_acc'], marker='s', label='Fine-tuning')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Test Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nFinal Comparison:')\n",
    "print(f'Feature Extraction - Best Test Acc: {max(history_fe[\"test_acc\"]):.4f}')\n",
    "print(f'Fine-tuning - Best Test Acc: {max(history_ft[\"test_acc\"]):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, dataloader):\n",
    "    \"\"\"\n",
    "    Get all predictions and true labels\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc='Evaluating'):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return predictions, true_labels\n",
    "\n",
    "# Get predictions for fine-tuned model\n",
    "predictions, true_labels = get_predictions(finetuning_model, test_loader)\n",
    "\n",
    "# Classification report\n",
    "print('Classification Report (Fine-tuned Model):\\n')\n",
    "print(classification_report(true_labels, predictions, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "           xticklabels=class_names, yticklabels=class_names)\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix (Fine-tuned Model)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of test images\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = next(dataiter)\n",
    "images = images.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "# Make predictions\n",
    "finetuning_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = finetuning_model(images)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "# Display predictions\n",
    "fig, axes = plt.subplots(2, 4, figsize=(15, 8))\n",
    "mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "std = torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = images[i].cpu().clone()\n",
    "    img = denormalize(img, mean, std)\n",
    "    img = torch.clamp(img, 0, 1)\n",
    "    \n",
    "    ax.imshow(img.permute(1, 2, 0))\n",
    "    \n",
    "    true_label = class_names[labels[i]]\n",
    "    pred_label = class_names[preds[i]]\n",
    "    color = 'green' if labels[i] == preds[i] else 'red'\n",
    "    \n",
    "    ax.set_title(f'True: {true_label}\\nPred: {pred_label}', color=color)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_path = './finetuned_resnet18_cifar10.pth'\n",
    "torch.save(finetuning_model.state_dict(), model_path)\n",
    "print(f'Model saved to {model_path}')\n",
    "\n",
    "# To load the model later:\n",
    "# model = models.resnet18()\n",
    "# model.fc = nn.Linear(model.fc.in_features, len(class_names))\n",
    "# model.load_state_dict(torch.load(model_path))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Transfer Learning Strategies:\n",
    "\n",
    "1. **Feature Extraction**:\n",
    "   - Freeze all pre-trained layers\n",
    "   - Only train new classification layer\n",
    "   - Fast training, works well with limited data\n",
    "   - Good when source and target domains are similar\n",
    "\n",
    "2. **Fine-tuning**:\n",
    "   - Unfreeze some layers (usually later layers)\n",
    "   - Train unfrozen layers with smaller learning rate\n",
    "   - Better performance but requires more data and time\n",
    "   - Better when domains are somewhat different\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. **Learning Rates**: Use smaller learning rates for pre-trained layers\n",
    "2. **Data Augmentation**: Important for preventing overfitting\n",
    "3. **Gradual Unfreezing**: Start with frozen layers, gradually unfreeze\n",
    "4. **Normalization**: Use same statistics as pre-training dataset\n",
    "\n",
    "### When to Use Transfer Learning:\n",
    "\n",
    "- Limited training data\n",
    "- Similar tasks (e.g., both are image classification)\n",
    "- Want faster convergence\n",
    "- Need better performance with less computational cost\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try different pre-trained models (VGG, EfficientNet, etc.)\n",
    "- Experiment with different freezing strategies\n",
    "- Apply to your own custom datasets\n",
    "- Compare with training from scratch\n",
    "- Explore domain adaptation techniques"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
