{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Large Language Models (LLMs) with Benchmark Datasets\n",
    "\n",
    "This notebook demonstrates how to use pre-trained Large Language Models (LLMs) for text generation and analysis on the IMDB movie review dataset, a standard benchmark for sentiment analysis.\n",
    "\n",
    "## What are LLMs?\n",
    "\n",
    "Large Language Models are neural networks trained on vast amounts of text data. They learn to understand and generate human-like text. Examples include GPT-2, GPT-3, BERT, and many others.\n",
    "\n",
    "## IMDB Dataset\n",
    "\n",
    "The IMDB dataset contains 50,000 movie reviews labeled as positive or negative, making it a benchmark for sentiment analysis tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel, \n",
    "    GPT2Tokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    pipeline\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Text Generation with GPT-2\n",
    "\n",
    "Let's start by using GPT-2, a popular generative LLM, to generate movie review-style text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 model and tokenizer\n",
    "print('Loading GPT-2 model...')\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "gpt2_model = gpt2_model.to(device)\n",
    "gpt2_model.eval()\n",
    "\n",
    "# Set pad token\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "\n",
    "print(f'Model parameters: {sum(p.numel() for p in gpt2_model.parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text\n",
    "def generate_text(prompt, max_length=100, num_return_sequences=1):\n",
    "    \"\"\"\n",
    "    Generate text using GPT-2\n",
    "    \"\"\"\n",
    "    inputs = gpt2_tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = gpt2_model.generate(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            no_repeat_ngram_size=2,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.8,\n",
    "            do_sample=True,\n",
    "            pad_token_id=gpt2_tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_texts = []\n",
    "    for output in outputs:\n",
    "        text = gpt2_tokenizer.decode(output, skip_special_tokens=True)\n",
    "        generated_texts.append(text)\n",
    "    \n",
    "    return generated_texts\n",
    "\n",
    "# Generate movie review-style text\n",
    "prompts = [\n",
    "    \"This movie was absolutely\",\n",
    "    \"I really enjoyed the film because\",\n",
    "    \"The acting in this movie was\"\n",
    "]\n",
    "\n",
    "print(\"Generated Movie Review Texts:\\n\")\n",
    "for prompt in prompts:\n",
    "    generated = generate_text(prompt, max_length=80)\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(f\"Generated: {generated[0]}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Load IMDB Benchmark Dataset\n",
    "\n",
    "Now let's load the IMDB dataset to analyze real movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IMDB dataset\n",
    "print('Loading IMDB dataset...')\n",
    "imdb_dataset = load_dataset('imdb', split='test[:1000]')  # Load subset for faster demo\n",
    "\n",
    "print(f'Dataset size: {len(imdb_dataset)}')\n",
    "print(f'Features: {imdb_dataset.features}\\n')\n",
    "\n",
    "# Show sample reviews\n",
    "print('Sample reviews:')\n",
    "for i in range(3):\n",
    "    review = imdb_dataset[i]\n",
    "    label = 'Positive' if review['label'] == 1 else 'Negative'\n",
    "    print(f\"\\nReview {i+1} ({label}):\")\n",
    "    print(review['text'][:200] + '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Sentiment Analysis with Pre-trained LLM\n",
    "\n",
    "Let's use a pre-trained BERT-based model fine-tuned for sentiment analysis on IMDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sentiment analysis pipeline\n",
    "print('Loading sentiment analysis model...')\n",
    "sentiment_pipeline = pipeline(\n",
    "    'sentiment-analysis',\n",
    "    model='distilbert-base-uncased-finetuned-sst-2-english',\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "print('Model loaded successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sentiment of sample reviews\n",
    "num_samples = 100\n",
    "correct = 0\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "print(f'Analyzing {num_samples} reviews...')\n",
    "for i in tqdm(range(num_samples)):\n",
    "    review = imdb_dataset[i]\n",
    "    text = review['text'][:512]  # Truncate to max length\n",
    "    true_label = review['label']\n",
    "    \n",
    "    # Get prediction\n",
    "    result = sentiment_pipeline(text)[0]\n",
    "    pred_label = 1 if result['label'] == 'POSITIVE' else 0\n",
    "    \n",
    "    predictions.append(pred_label)\n",
    "    true_labels.append(true_label)\n",
    "    \n",
    "    if pred_label == true_label:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / num_samples\n",
    "print(f'\\nAccuracy on {num_samples} samples: {accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Negative', 'Positive'],\n",
    "            yticklabels=['Negative', 'Positive'])\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title(f'Confusion Matrix (Accuracy: {accuracy:.2%})')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(true_labels, predictions, \n",
    "                          target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Analyze Specific Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some predictions with confidence scores\n",
    "print('Sample Predictions with Confidence Scores:\\n')\n",
    "for i in range(5):\n",
    "    review = imdb_dataset[i]\n",
    "    text = review['text'][:200]  # Show first 200 chars\n",
    "    true_label = 'Positive' if review['label'] == 1 else 'Negative'\n",
    "    \n",
    "    result = sentiment_pipeline(review['text'][:512])[0]\n",
    "    pred_label = result['label']\n",
    "    confidence = result['score']\n",
    "    \n",
    "    print(f\"Review {i+1}:\")\n",
    "    print(f\"Text: {text}...\")\n",
    "    print(f\"True: {true_label} | Predicted: {pred_label} (confidence: {confidence:.2%})\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **LLM Capabilities**: Large Language Models can both generate text (GPT-2) and analyze text (BERT)\n",
    "2. **Pre-trained Models**: Using pre-trained models saves significant time and computational resources\n",
    "3. **Benchmark Datasets**: IMDB is a standard benchmark for evaluating sentiment analysis models\n",
    "4. **Transfer Learning**: Models trained on general text can be fine-tuned for specific tasks\n",
    "5. **Zero-shot vs Fine-tuned**: Fine-tuned models typically perform better on specific tasks\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Explore other LLMs like BERT, RoBERTa, or T5\n",
    "- Try different benchmark datasets (SST-2, Amazon Reviews, etc.)\n",
    "- See the fine-tuning notebook to learn how to adapt models for specific tasks\n",
    "- Experiment with different generation parameters for GPT-2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
