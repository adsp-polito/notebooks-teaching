{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Vision Transformer (ViT) Foundation Model with CIFAR-10\n",
    "\n",
    "This notebook demonstrates how to use a pre-trained Vision Transformer (ViT) foundation model for image classification on the CIFAR-10 benchmark dataset.\n",
    "\n",
    "## What is a Foundation Model?\n",
    "\n",
    "Foundation models are large-scale pre-trained models that can be adapted to various downstream tasks. Vision Transformers (ViT) were introduced by Google Research and have become a popular foundation model for computer vision tasks.\n",
    "\n",
    "## CIFAR-10 Dataset\n",
    "\n",
    "CIFAR-10 is a well-known benchmark dataset consisting of 60,000 32x32 color images in 10 classes, with 6,000 images per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CIFAR-10 Dataset\n",
    "\n",
    "We'll load the CIFAR-10 dataset using torchvision. The dataset will be automatically downloaded if not present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10 class names\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # ViT expects 224x224 images\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Download and load test dataset\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, \n",
    "                                download=True, transform=transform)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f'Test dataset size: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Sample Images\n",
    "\n",
    "Let's visualize a few sample images from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to denormalize images for visualization\n",
    "def denormalize(img):\n",
    "    img = img * 0.5 + 0.5  # Denormalize from [-1, 1] to [0, 1]\n",
    "    return img\n",
    "\n",
    "# Display sample images\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img, label = test_dataset[i]\n",
    "    img = denormalize(img)\n",
    "    ax.imshow(img.permute(1, 2, 0))\n",
    "    ax.set_title(class_names[label])\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-trained Vision Transformer Model\n",
    "\n",
    "We'll use a pre-trained ViT model from Hugging Face's transformers library. This model has been pre-trained on ImageNet-21k and can be used for zero-shot classification or fine-tuned for specific tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained ViT model and processor\n",
    "model_name = 'google/vit-base-patch16-224'\n",
    "processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "model = ViTForImageClassification.from_pretrained(model_name)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f'Model loaded: {model_name}')\n",
    "print(f'Number of parameters: {sum(p.numel() for p in model.parameters()):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions\n",
    "\n",
    "Let's use the foundation model to make predictions on a few test images. Note that the model was trained on ImageNet, so the predictions may not perfectly align with CIFAR-10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict on a single image\n",
    "def predict_image(img_tensor, model, processor):\n",
    "    # Denormalize and convert to PIL format expected by processor\n",
    "    img = denormalize(img_tensor)\n",
    "    img_pil = transforms.ToPILImage()(img)\n",
    "    \n",
    "    # Process image and make prediction\n",
    "    inputs = processor(images=img_pil, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_class_idx = logits.argmax(-1).item()\n",
    "    \n",
    "    return predicted_class_idx, logits\n",
    "\n",
    "# Make predictions on sample images\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 8))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img, label = test_dataset[i]\n",
    "    pred_idx, logits = predict_image(img, model, processor)\n",
    "    \n",
    "    # Get predicted class name from ImageNet labels\n",
    "    predicted_label = model.config.id2label[pred_idx]\n",
    "    \n",
    "    img_display = denormalize(img)\n",
    "    ax.imshow(img_display.permute(1, 2, 0))\n",
    "    ax.set_title(f'True: {class_names[label]}\\nPred: {predicted_label[:20]}...', \n",
    "                 fontsize=8)\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance\n",
    "\n",
    "Let's evaluate the model's performance on a subset of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on a subset (100 images) for demonstration\n",
    "num_samples = 100\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "print(f'Evaluating on {num_samples} images...')\n",
    "for i in tqdm(range(num_samples)):\n",
    "    img, label = test_dataset[i]\n",
    "    pred_idx, _ = predict_image(img, model, processor)\n",
    "    \n",
    "    # Note: This is approximate since ImageNet and CIFAR-10 classes don't match exactly\n",
    "    # In practice, you would need to fine-tune the model for CIFAR-10\n",
    "    total += 1\n",
    "\n",
    "print(f'\\nNote: Direct evaluation is challenging because the model was trained on ImageNet,')\n",
    "print('which has different classes than CIFAR-10. For accurate results, the model should be fine-tuned.')\n",
    "print('See the fine-tuning notebook for an example of adapting this model to CIFAR-10.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Foundation Models**: Vision Transformers are powerful foundation models pre-trained on large datasets (ImageNet-21k)\n",
    "2. **Zero-Shot Capabilities**: These models can make predictions out-of-the-box, though they work best on classes similar to their training data\n",
    "3. **Transfer Learning Ready**: The model can be fine-tuned for specific tasks like CIFAR-10 classification\n",
    "4. **Benchmark Datasets**: CIFAR-10 is a standard benchmark for evaluating image classification models\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- See the fine-tuning notebook to learn how to adapt this model specifically for CIFAR-10\n",
    "- Explore other foundation models like CLIP, DINO, or Swin Transformers\n",
    "- Try different benchmark datasets like CIFAR-100, ImageNet, or domain-specific datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
